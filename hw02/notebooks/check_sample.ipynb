{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/.pyenv/versions/3.8.12/envs/adl-hw02/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: swag/regular\n",
      "Reusing dataset swag (/home/jacky/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c)\n",
      "100%|██████████| 3/3 [00:00<00:00, 60.59it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"swag\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_pIUpJihiju0',\n",
       " 'fold-ind': '11871',\n",
       " 'startphrase': 'A person shows the bottom of a large dust mop. Then, the person',\n",
       " 'sent1': 'A person shows the bottom of a large dust mop.',\n",
       " 'sent2': 'Then, the person',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'places the scrub on the floor then cleans the soap evenly.',\n",
       " 'ending1': 'cleans a gym with the large dust mop.',\n",
       " 'ending2': 'stops and spray a car with some spray paint.',\n",
       " 'ending3': 'places the bucket and a bucket.',\n",
       " 'label': -1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=True,\n",
    "    model_revision=\"main\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "ending_names = [f\"ending{i}\" for i in range(4)]\n",
    "context_name = \"sent1\"\n",
    "question_header_name = \"sent2\"\n",
    "\n",
    "max_seq_length = min(tokenizer.model_max_length, 1024)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[context_name]]\n",
    "    question_headers = examples[question_header_name]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    # Flatten out\n",
    "    first_sentences = list(chain.from_iterable(first_sentences))\n",
    "    second_sentences = list(chain.from_iterable(second_sentences))\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=False,\n",
    "    )\n",
    "    # Un-flatten\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jacky/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c/cache-7c46fde38bb83186.arrow\n",
      "100%|██████████| 74/74 [00:13<00:00,  5.45ba/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 20006\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'lsmdc1052_Harry_Potter_and_the_order_of_phoenix-94857',\n",
       " 'fold-ind': '18313',\n",
       " 'startphrase': 'Students lower their eyes nervously. She',\n",
       " 'sent1': 'Students lower their eyes nervously.',\n",
       " 'sent2': 'She',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'pats her shoulder, then saunters toward someone.',\n",
       " 'ending1': 'turns with two students.',\n",
       " 'ending2': 'walks slowly towards someone.',\n",
       " 'ending3': 'wheels around as her dog thunders out.',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.tokenization_utils import PaddingStrategy\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = list(chain.from_iterable(flattened_features))\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForMultipleChoice(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "features = [valid_dataset[i] for i in range(3)]\n",
    "label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "labels = [feature.pop(label_name) for feature in features]\n",
    "batch_size = len(features)\n",
    "num_choices = len(features[0][\"input_ids\"])\n",
    "cols = (\"input_ids\", \"attention_mask\")\n",
    "flattened_features = [\n",
    "    [{col: feature[col][i] for col in cols} for i in range(num_choices)] for feature in features\n",
    "]\n",
    "flattened_features = list(chain.from_iterable(flattened_features))\n",
    "batch = tokenizer.pad(\n",
    "    flattened_features,\n",
    "    padding=True,\n",
    "    max_length=None,\n",
    "    pad_to_multiple_of=None,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    from_tf=False,\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments( output_dir=\"/tmp/swag_base\", no_cuda=True),\n",
    "    train_dataset=train_dataset.select([0, 10, 20, 30, 40, 50]),\n",
    "    eval_dataset=valid_dataset.select([0, 10, 20, 30, 40, 50]),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMultipleChoice.forward` and have been ignored: startphrase, fold-ind, ending3, video-id, ending1, sent1, ending2, sent2, ending0, gold-source. If startphrase, fold-ind, ending3, video-id, ending1, sent1, ending2, sent2, ending0, gold-source are not expected by `RobertaForMultipleChoice.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.38624906539917,\n",
       " 'eval_accuracy': 0.3333333432674408,\n",
       " 'eval_runtime': 0.3616,\n",
       " 'eval_samples_per_second': 16.593,\n",
       " 'eval_steps_per_second': 2.766}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e88d3854f1c4b43895a586a5853165219acc8ff0d8d840d9d7406f6022443eab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('adl-hw02')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
