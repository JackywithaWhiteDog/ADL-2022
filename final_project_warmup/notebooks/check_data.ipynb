{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_metric, load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_input_length = None\n",
    "max_target_length = None\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['inputs']]\n",
    "    targets = [ex for ex in examples['target']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, truncation=True, padding=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=max_target_length, truncation=True, padding=False,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "class OTTersDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, data):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.encodings = tokenizer(data['inputs'], padding=True, truncation=True) \n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            self.targets = tokenizer(data['targets'], padding=True, truncation=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.targets['input_ids'][idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "def read_data(data_dir):\n",
    "    splits = ['train', 'dev', 'test']\n",
    "    datasets = {}\n",
    "    for split in splits:\n",
    "        directory = os.path.join(data_dir, split)\n",
    "        datasets[split] = load_dataset(directory, data_files=['text.csv'])\n",
    "        if split != 'test':\n",
    "            datasets[split] = datasets[split].map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                remove_columns=['inputs', 'target'],\n",
    "            )['train']\n",
    "        else:\n",
    "            datasets[split] = datasets[split]['train']\n",
    "    return datasets['train'], datasets['dev'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration train-38be866dbae812fe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/train to /home/jacky/.cache/huggingface/datasets/csv/train-38be866dbae812fe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2616.53it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 326.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jacky/.cache/huggingface/datasets/csv/train-38be866dbae812fe/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 436.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  7.45ba/s]\n",
      "Using custom data configuration dev-fe834cd7bb2c5fba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/dev to /home/jacky/.cache/huggingface/datasets/csv/dev-fe834cd7bb2c5fba/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1419.87it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 434.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jacky/.cache/huggingface/datasets/csv/dev-fe834cd7bb2c5fba/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 528.98it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 29.09ba/s]\n",
      "Using custom data configuration test-e401684cd5ad938c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/test to /home/jacky/.cache/huggingface/datasets/csv/test-e401684cd5ad938c/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2964.17it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 781.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jacky/.cache/huggingface/datasets/csv/test-e401684cd5ad938c/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 318.89it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../data/out_of_domain'\n",
    "train_dataset, eval_dataset, test_dataset = read_data(dataset_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([len(d) for d in train_dataset['input_ids']] + [len(d) for d in eval_dataset['input_ids']]) <= 50).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([len(d) for d in train_dataset['labels']] + [len(d) for d in eval_dataset['labels']]) <= 75).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5897305dd15b6b6db08a7a81208839b999dd15af6e23f10591dc8894db04da27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('adl-hw03')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
