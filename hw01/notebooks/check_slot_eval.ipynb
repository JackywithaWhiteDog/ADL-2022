{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "from src.slot.data_manager import SlotDataManager\n",
    "from src.slot.models import SlotTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 03:04:09 | INFO | Vocab loaded from /home/jacky/110-2_ADL/homeworks/hw01/cache/slot/vocab.pkl\n",
      "2022-03-18 03:04:09 | INFO | Tag-2-Index loaded from /home/jacky/110-2_ADL/homeworks/hw01/cache/slot/tag2idx.json\n",
      "2022-03-18 03:04:09 | INFO | Embeddings loaded from /home/jacky/110-2_ADL/homeworks/hw01/cache/slot/embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "data_manager = SlotDataManager(\n",
    "    cache_dir=Path(\"../cache/slot\"),\n",
    "    max_len=128,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    data_dir=Path(\"../data/slot\"),\n",
    "    test_file=Path(\"../data/slot/test.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacky/.pyenv/versions/3.8.12/envs/adl-hw01/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlotTagger(\n",
       "  (embedding): Embedding(3002, 300, padding_idx=0)\n",
       "  (rnn): GRU(300, 128, dropout=0.5, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=9, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SlotTagger.load_from_checkpoint(Path(\"../upload/ckpt/slot/slot-best.ckpt\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1123\n",
      "100%|██████████| 32/32 [00:00<00:00, 38.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 33, 9]),\n",
       " torch.Size([1000, 33]),\n",
       " torch.Size([1000]),\n",
       " torch.Size([7891, 9]),\n",
       " torch.Size([7891]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1123)\n",
    "valid_dataloader = data_manager.get_valid_dataloader()\n",
    "length_list = []\n",
    "output_list = []\n",
    "y_list = []\n",
    "flatten_output_list = []\n",
    "flatten_y_list = []\n",
    "for x, length, y in tqdm(valid_dataloader):\n",
    "    output = model(x, length)\n",
    "    length_list.append(length)\n",
    "    output_list.append(output)\n",
    "    y_list.append(y)\n",
    "    flatten_output_list.append(torch.cat([\n",
    "        sen_output[:sen_len, :]\n",
    "        for sen_output, sen_len, in zip(output, length)\n",
    "    ]))\n",
    "    flatten_y_list.append(torch.cat([\n",
    "        sen_tags[:sen_len]\n",
    "        for sen_tags, sen_len, in zip(y, length)\n",
    "    ]))\n",
    "length = torch.cat(length_list)\n",
    "max_len = length.max()\n",
    "output = torch.cat([\n",
    "    F.pad(o, (0, 0, 0, max_len - o.shape[1]))\n",
    "    for o in output_list\n",
    "])\n",
    "y = torch.cat(y_list)\n",
    "flatten_output = torch.cat(flatten_output_list)\n",
    "flatten_y = torch.cat(flatten_y_list)\n",
    "output.shape, y.shape, length.shape, flatten_output.shape, flatten_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Accuracy: 0.9618552923202515\n"
     ]
    }
   ],
   "source": [
    "token_acc = model.token_acc(pred=flatten_output, target=flatten_y)\n",
    "print(f\"Token Accuracy: {token_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join Accuracy: 0.7730000019073486\n"
     ]
    }
   ],
   "source": [
    "join_acc = model.join_acc(pred=output, length=length, target=y)\n",
    "print(f\"Join Accuracy: {join_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_y = [\n",
    "    [data_manager.idx2tag[idx] for idx in sen_tags[:sen_len].tolist()]\n",
    "    for sen_tags, sen_len in zip(y, length)\n",
    "]\n",
    "clipped_pred = [\n",
    "    [data_manager.idx2tag[idx] for idx in sen_val[:sen_len].argmax(dim=1).tolist()]\n",
    "    for sen_val, sen_len in  zip(output, length)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        date       0.72      0.74      0.73       206\n",
      "  first_name       0.94      0.91      0.93       102\n",
      "   last_name       0.89      0.81      0.85        78\n",
      "      people       0.69      0.68      0.68       238\n",
      "        time       0.81      0.79      0.80       218\n",
      "\n",
      "   micro avg       0.78      0.76      0.77       842\n",
      "   macro avg       0.81      0.78      0.80       842\n",
      "weighted avg       0.78      0.76      0.77       842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=clipped_y, y_pred=clipped_pred, scheme=IOB2, mode=\"strict\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 6458,\n",
       "         'B-time': 218,\n",
       "         'B-people': 238,\n",
       "         'B-date': 206,\n",
       "         'I-date': 290,\n",
       "         'B-first_name': 102,\n",
       "         'B-last_name': 78,\n",
       "         'I-people': 231,\n",
       "         'I-time': 70})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(tag for tags in clipped_y for tag in tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 6475,\n",
       "         'B-time': 213,\n",
       "         'B-people': 234,\n",
       "         'B-date': 210,\n",
       "         'I-date': 276,\n",
       "         'I-people': 234,\n",
       "         'B-first_name': 99,\n",
       "         'B-last_name': 71,\n",
       "         'I-time': 79})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tag for tags in clipped_pred for tag in tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "def get_chunk(tags: List[str]) -> List[Tuple[str, int, int]]:\n",
    "    result = []\n",
    "    current = None\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag[:2] != \"I-\" and current is not None:\n",
    "            result.append((current[0], current[1], i-1))\n",
    "            current = None\n",
    "        if tag[:2] == \"B-\":\n",
    "            current = (tag[2:], i)\n",
    "    if current is not None:\n",
    "        result.append((current[0], current[1], i))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_y = [get_chunk(tags) for tags in clipped_y]\n",
    "chunked_pred = [get_chunk(tags) for tags in clipped_pred]\n",
    "len(chunked_y), len(chunked_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set(chunk[0] for chunks in chunked_y for chunk in chunks) | set(chunk[0] for chunks in chunked_pred for chunk in chunks)\n",
    "result = {\n",
    "    tag: {\n",
    "        \"tp\": 0,\n",
    "        \"fp\": 0,\n",
    "        \"fn\": 0,\n",
    "        \"support\": 0,\n",
    "    }\n",
    "    for tag in tags\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_chunks, pred_chunks in zip(chunked_y, chunked_pred):\n",
    "    for chunk in y_chunks:\n",
    "        tag = chunk[0]\n",
    "        result[tag][\"support\"] += 1\n",
    "        if chunk in pred_chunks:\n",
    "            result[tag][\"tp\"] += 1\n",
    "        else:\n",
    "            result[tag][\"fn\"] += 1\n",
    "    for chunk in pred_chunks:\n",
    "        tag = chunk[0]\n",
    "        if chunk not in y_chunks:\n",
    "            result[tag][\"fp\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    tag: {\n",
    "        \"precision\": value[\"tp\"] / (value[\"tp\"] + value[\"fp\"]),\n",
    "        \"recall\": value[\"tp\"] / (value[\"tp\"] + value[\"fn\"]),\n",
    "        **value\n",
    "    }\n",
    "    for tag, value in result.items()\n",
    "}\n",
    "result = {\n",
    "    tag: {\n",
    "        \"f1\": 2 * value[\"precision\"] * value[\"recall\"] / (value[\"precision\"] + value[\"recall\"]),\n",
    "        **value\n",
    "    }\n",
    "    for tag, value in result.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support: 842\n"
     ]
    }
   ],
   "source": [
    "print(f'Support: {sum(value[\"support\"] for value in result.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision (micro): 0.7750906892382105\n",
      "recall (micro): 0.7612826603325415\n",
      "f1 (micro): 0.768124625524266\n"
     ]
    }
   ],
   "source": [
    "metrics = (\"precision\", \"recall\", \"f1\")\n",
    "tp = sum(value[\"tp\"] for value in result.values())\n",
    "fp = sum(value[\"fp\"] for value in result.values())\n",
    "fn = sum(value[\"fn\"] for value in result.values())\n",
    "micro = {\n",
    "    \"precision\": tp / (tp + fp),\n",
    "    \"recall\": tp / (tp + fn)\n",
    "}\n",
    "micro[\"f1\"] = 2 * micro[\"precision\"] * micro[\"recall\"] / (micro[\"precision\"] + micro[\"recall\"])\n",
    "for m in metrics:\n",
    "    print(f\"{m} (micro): {micro[m]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision (macro): 0.8092146663977651\n",
      "recall (macro): 0.7845565010335862\n",
      "f1 (macro): 0.7964254380657672\n"
     ]
    }
   ],
   "source": [
    "metrics = (\"precision\", \"recall\", \"f1\")\n",
    "for m in metrics:\n",
    "    print(f\"{m} (macro): {sum(value[m] for value in result.values()) / len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision (weighted): 0.7766317182495965\n",
      "recall (weighted): 0.7612826603325415\n",
      "f1 (weighted): 0.7687007353824342\n"
     ]
    }
   ],
   "source": [
    "metrics = (\"precision\", \"recall\", \"f1\")\n",
    "for m in metrics:\n",
    "    print(f\"{m} (weighted): {sum(value[m] * value['support'] for value in result.values()) / sum(value['support'] for value in result.values())}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14e17195916193df421607a98b0ec24e3c430e4b809de709985372f663764d14"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('adl-hw01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
